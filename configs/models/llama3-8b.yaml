# Llama 3 8B configuration

model:
  # Model identifier
  name: "meta-llama/Llama-3-8b"  # Placeholder name
  type: "causal"
  
  # Inherit default configurations
  <<: *model_defaults
  
  # Model-specific configurations
  model_kwargs:
    device_map: "auto"
    load_in_4bit: true
    max_memory:
      0: "24GiB"  # Adjust based on your GPU memory
  
  # Generation configurations
  generation:
    max_new_tokens: 4096  # Llama models typically support longer sequences
    temperature: 0.7
    top_p: 0.9
    repetition_penalty: 1.1
    do_sample: true
